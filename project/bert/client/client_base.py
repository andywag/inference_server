# Copyright (c) 2020, NVIDIA CORPORATION. All rights reserved.
#
# Redistribution and use in source and binary forms, with or without
# modification, are permitted provided that the following conditions
# are met:
#  * Redistributions of source code must retain the above copyright
#    notice, this list of conditions and the following disclaimer.
#  * Redistributions in binary form must reproduce the above copyright
#    notice, this list of conditions and the following disclaimer in the
#    documentation and/or other materials provided with the distribution.
#  * Neither the name of NVIDIA CORPORATION nor the names of its
#    contributors may be used to endorse or promote products derived
#    from this software without specific prior written permission.
#
# THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS ``AS IS'' AND ANY
# EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE
# IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR
# PURPOSE ARE DISCLAIMED.  IN NO EVENT SHALL THE COPYRIGHT OWNER OR
# CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL,
# EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO,
# PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR
# PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY
# OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT
# (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE
# OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.

from tritonclient.utils import *
import tritonclient.grpc as grpcclient
import argparse
import subprocess

import sys
import os
sys.path.append("../run")

import numpy as np
import time
import mlperf_loadgen as lg
from   squad_QSL import get_squad_QSL
import math
import numpy as np
import array
import threading
import queue

import logging
log = logging.getLogger(__name__)
log.setLevel(logging.INFO)
#logging.basicConfig(format='[%(filename)s:%(lineno)d] %(message)s', level=logging.DEBUG)

model_name = "squad"
shape = [4]

def get_args():
    parser = argparse.ArgumentParser()
    parser.add_argument("--mlperf_conf", default="../run/build/mlperf.conf", help="mlperf rules config")
    parser.add_argument("--max_examples", type=int, help="Maximum number of examples to consider (not limited by default)")

    parser.add_argument("--accuracy", action="store_true", help="enable accuracy pass")
    parser.add_argument("--features_cache_path", default="eval_features.pickle", help="Path to the pickled file that contains tokenized features")
    parser.add_argument("--vocab_file_path", default="../run/build/data/bert_tf_v1_1_large_fp32_384_v2/vocab.txt", help="Path to the tokenization vocabulary file")
    parser.add_argument("--orig_squad_path", default="../run/build/data/dev-v1.1.json", help="Path to the original SQuAD 1.1 database (JSON)")
    parser.add_argument("--accuracy_predictions_path", default="../run/build/result/predictions.json", help="Path to the generated predictions file (JSON)")
    parser.add_argument("--log_path", default="logs", help="Path to the log files generated by LoadGen")

    parser.add_argument("--threads", type=int, default=16, help="Path to the log files generated by LoadGen")
    parser.add_argument("--groups", type=int, default=128, help="Path to the log files generated by LoadGen")
    parser.add_argument("--number_backends", type=int, default=4, help="Path to the log files generated by LoadGen")

    args = parser.parse_args()

    return args

def run_loadgen(args, SUT, server=False):


    log.info("Starting LoadGen Test")

    settings = lg.TestSettings()
    
    scenario = "Offline"
    settings.scenario = lg.TestScenario.Offline
    if server:
        scenario = "Server"
        settings.scenario = lg.TestScenario.Server

    settings.FromConfig(args.mlperf_conf, "bert", scenario)
    settings.FromConfig("user.conf", "bert", scenario)
    if args.accuracy:
        settings.mode = lg.TestMode.AccuracyOnly
    else:
        settings.mode = lg.TestMode.PerformanceOnly
    if not os.path.exists(args.log_path):
        os.makedirs(args.log_path)
    log_output_settings = lg.LogOutputSettings()
    log_output_settings.outdir = args.log_path
    log_output_settings.copy_summary_to_stdout = True
    log_settings = lg.LogSettings()
    log_settings.log_output = log_output_settings
    log_settings.enable_trace = False

    qsl = get_squad_QSL(None, None, args.features_cache_path, build_path = "../run/build")
    sut = SUT(qsl, lg, number_threads=args.threads, groups=args.groups, number_backends=args.number_backends)
    lg_sut = lg.ConstructSUT(sut.issue_queries, sut.flush_queries, sut.process_latencies)

    sut.start_time = time.time()
    lg.StartTestWithLogSettings(lg_sut, qsl.qsl, settings, log_settings)
    # Hack to allow pipeline to close 
    time.sleep(10)
       
    log.info('Finished Load Gen Test')

    if args.accuracy and args.accuracy_predictions_path:
        cmd = 'python3 ' + os.path.dirname(os.path.abspath(__file__))+'/../run/accuracy-squad.py'
        accuracy_option_map = {
            'max_examples':                 'max_examples',
            'features_cache_path':          'features_cache_file',
            'vocab_file_path':              'vocab_file',
            'orig_squad_path':              'val_data',
            'accuracy_predictions_path':    'out_file',
        }
        for option_name in accuracy_option_map:
            option_value = getattr(args, option_name)
            if option_value:
                cmd += f" --{accuracy_option_map[option_name]}={option_value}"

        if args.log_path:
            cmd += f" --log_file={args.log_path}/mlperf_log_accuracy.json"

        subprocess.check_call(cmd, shell=True)

    print("Done!")

    print("Destroying SUT...")
    lg.DestroySUT(lg_sut)

    print("Destroying QSL...")
    lg.DestroyQSL(qsl.qsl)


def first_non_zero(in_data):
    for x in range(len(in_data)):
        if in_data[x] == 1:
            return x
    return 384

class SUTBase:
    def __init__(self, qsl, lg, number_threads = 2, groups = 128, number_backends=4):
        self.qsl = qsl
        self.lg = lg

        self.input_count = 0
        self.output_count = 0
        self.total_count = 0

        self.input_queue = queue.Queue()

        self.number_threads = number_threads
        self.number_backends = number_backends
        self.groups = groups
        
        self.report_time = 8*self.number_threads
        self.client = grpcclient.InferenceServerClient("localhost:8001")
    
        self.total_tx_query = 0
        self.total_rx_query = 0

    def create_input_samples(self, query_samples):
        self.input_count += 1
        #print("Send Data", len(query_samples))
        number_queries = len(query_samples)
        input_ids = np.zeros((number_queries, 384), dtype=np.uint32)
        segment_ids = np.zeros((number_queries, 1), dtype=np.uint32)
        query_ids = np.zeros((number_queries, 1), dtype=np.uint64)

        for i,q  in enumerate(query_samples):
            index = query_samples[i].index
            eval_features = self.qsl.get_features(index)
            input_ids[i,:] = np.asarray(eval_features.input_ids)
            segment_ids[i] = first_non_zero(eval_features.segment_ids)
            query_ids[i] = query_samples[i].id

        return input_ids, segment_ids, query_ids

    def create_request(self, query_samples):
        input_ids, segment_ids, query_ids = self.create_input_samples(query_samples)

        inputs = [
                grpcclient.InferInput("input_ids", input_ids.shape,
                                    np_to_triton_dtype(input_ids.dtype)),
                grpcclient.InferInput("segment_ids", segment_ids.shape,
                                    np_to_triton_dtype(segment_ids.dtype)),
                grpcclient.InferInput("query_ids", query_ids.shape,
                                    np_to_triton_dtype(query_ids.dtype)),
        ]
        
        inputs[0].set_data_from_numpy(input_ids)
        inputs[1].set_data_from_numpy(segment_ids)
        inputs[2].set_data_from_numpy(query_ids)

        outputs = [
                grpcclient.InferRequestedOutput("logits"),
                grpcclient.InferRequestedOutput("query_ids_result"),
        ]
        #print("Start", query_ids)
        return inputs, outputs

    def handle_response_direct(self, logits, query_ids):
        #print("Response", logits.shape,query_ids.shape)
        for x in range(len(query_ids)):
            bl = logits[x]
            response_array = array.array("B", bl.tobytes())   # this is a bytes' array
            bi = response_array.buffer_info()
            #log.info(f"HHH {query_ids[x]}")
            response = self.lg.QuerySampleResponse(query_ids[x], bi[0], bi[1])
            self.lg.QuerySamplesComplete([response])

        self.output_count += 1
        output_count = self.output_count
        if output_count % self.report_time == self.report_time - 1:
            tic = time.time()
            qps = (self.input_count+1)*self.groups/(tic-self.start_time)
            log.info(f"Received {(output_count+1)*self.groups}/{self.total_count}") 
            log.info(f"Sent {(self.input_count+1)*self.groups}/{self.total_count} Queries {tic - self.start_time}, QPS {qps} : S-R {self.total_tx_query - self.total_rx_query}")


    def handle_response(self, response):
        logits = response.as_numpy("logits")
        query_ids = response.as_numpy("query_ids_result")
        self.handle_response_direct(logits, query_ids)
        #print("Response", logits, query_ids)


    def single_packet(self, query_samples, callback = None):
        self.total_tx_query += len(query_samples)

        inputs, outputs = self.create_request(query_samples)

        if self.input_count % self.report_time == self.report_time - 1:
            tic = time.time()
            qps = (self.input_count+1)*self.groups/(tic-self.start_time)
            log.info(f"Sent {(self.input_count+1)*self.groups}/{self.total_count} Queries {tic - self.start_time}, QPS {qps} : S-R {self.total_tx_query - self.total_rx_query}")

        if callback is None:
            response = self.client.infer(model_name,
                                inputs,
                                request_id=str(1),
                                outputs=outputs)
            self.handle_response(response)
        else:
            self.client.async_infer(model_name,
                            inputs,
                            request_id=str(1),
                            outputs=outputs, 
                            callback=callback)

       

       
    def flush_queries(self):
        pass
        #self.issue_query_single(self.last_id, self.last_feature, True)
        #print("flush queries")

    def process_latencies(self, latencies_ns):
        print("Handle Latency")

    def __del__(self):
        print("Finished destroying SUT.")


        #print(response.as_numpy("logits"))

def main(sut, server = False):
    args = get_args()
    run_loadgen(args, sut, server)

